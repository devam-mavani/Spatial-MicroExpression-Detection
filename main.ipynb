{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of GPUs: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "print('No. of GPUs:', len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image, target_size=(200, 200)):\n",
    "    resized_image = cv2.resize(image, target_size, interpolation=cv2.INTER_AREA)\n",
    "    normalized_image = np.array(resized_image, dtype=np.float32) / 255.0  # Normalize to [0, 1]\n",
    "    return normalized_image\n",
    "\n",
    "# Function to compute optical flow\n",
    "def compute_optical_flow(frames):\n",
    "    flows = []\n",
    "    for i in range(1, len(frames)):\n",
    "        prev_frame = frames[i-1]\n",
    "        next_frame = frames[i]\n",
    "        flow = cv2.calcOpticalFlowFarneback(prev_frame, next_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "        flows.append(flow)\n",
    "    return flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply optical flow and preprocess images\n",
    "def applyOpticalFlow(path, target_size=(200, 200)):\n",
    "    file_names = sorted(os.listdir(path))\n",
    "    spatial_features = []\n",
    "    for fn in file_names:\n",
    "        img = cv2.imread(f\"{path}/{fn}\", cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            preprocessed_img = preprocess_image(img, target_size)\n",
    "            spatial_features.append(preprocessed_img)\n",
    "    flows = compute_optical_flow(spatial_features)\n",
    "    return spatial_features, flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./casme/Cropped/Cropped/sub01/EP02_01f</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./casme/Cropped/Cropped/sub01/EP03_02</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./casme/Cropped/Cropped/sub01/EP04_02</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./casme/Cropped/Cropped/sub01/EP04_03</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./casme/Cropped/Cropped/sub01/EP04_04</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>./casme/Cropped/Cropped/sub01/EP19_01</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>./casme/Cropped/Cropped/sub01/EP19_03f</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>./casme/Cropped/Cropped/sub01/EP19_05f</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>./casme/Cropped/Cropped/sub01/EP19_06f</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>./casme/Cropped/Cropped/sub02/EP01_11f</td>\n",
       "      <td>repression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>./casme/Cropped/Cropped/sub02/EP02_04f</td>\n",
       "      <td>repression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>./casme/Cropped/Cropped/sub02/EP03_02f</td>\n",
       "      <td>repression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Path     Emotion\n",
       "0   ./casme/Cropped/Cropped/sub01/EP02_01f   happiness\n",
       "1    ./casme/Cropped/Cropped/sub01/EP03_02      others\n",
       "2    ./casme/Cropped/Cropped/sub01/EP04_02      others\n",
       "3    ./casme/Cropped/Cropped/sub01/EP04_03      others\n",
       "4    ./casme/Cropped/Cropped/sub01/EP04_04      others\n",
       "5    ./casme/Cropped/Cropped/sub01/EP19_01      others\n",
       "6   ./casme/Cropped/Cropped/sub01/EP19_03f      others\n",
       "7   ./casme/Cropped/Cropped/sub01/EP19_05f     disgust\n",
       "8   ./casme/Cropped/Cropped/sub01/EP19_06f     disgust\n",
       "9   ./casme/Cropped/Cropped/sub02/EP01_11f  repression\n",
       "10  ./casme/Cropped/Cropped/sub02/EP02_04f  repression\n",
       "11  ./casme/Cropped/Cropped/sub02/EP03_02f  repression"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./casme/CASME2-coding-20140508.csv').iloc[:12,:]\n",
    "rootPath = \"./casme/Cropped/Cropped\"\n",
    "X = []\n",
    "labels = []\n",
    "for index, row in df.iterrows():\n",
    "    X.append(f\"{rootPath}/sub{'0' if row['Subject'] < 10 else ''}{row['Subject']}/{row['Filename']}\")\n",
    "    labels.append(row['Estimated Emotion'])\n",
    "\n",
    "data = pd.DataFrame({'Path': X, 'Emotion': labels})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data\n",
    "spatial_features, motion_features = [], []\n",
    "for idx, row in data.iterrows():\n",
    "    spatial, motion = applyOpticalFlow(row['Path'])\n",
    "    spatial_features.append(spatial)\n",
    "    motion_features.append(motion)\n",
    "\n",
    "spatial_features = [seq for seq in spatial_features]\n",
    "motion_features = [seq for seq in motion_features]\n",
    "\n",
    "# Ensure all sequences have the same length\n",
    "max_length = max(len(seq) for seq in motion_features)\n",
    "motion_features = np.array([np.pad(seq, ((0, max_length - len(seq)), (0, 0), (0, 0), (0, 0)), mode='constant') for seq in motion_features], dtype=np.float32)\n",
    "\n",
    "print(len(labels))\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)\n",
    "num_classes = 4\n",
    "labels_one_hot = tf.keras.utils.to_categorical(labels, num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train_spatial, X_test_spatial, X_train_motion, X_test_motion, y_train, y_test = train_test_split(\n",
    "    spatial_features, motion_features, labels_one_hot, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to ensure they have the same shape\n",
    "X_train_spatial_padded = tf.keras.preprocessing.sequence.pad_sequences(X_train_spatial, padding='post', dtype='float32')\n",
    "X_train_motion_padded = tf.keras.preprocessing.sequence.pad_sequences(X_train_motion, padding='post', dtype='float32')\n",
    "X_test_spatial_padded = tf.keras.preprocessing.sequence.pad_sequences(X_test_spatial, padding='post', dtype='float32')\n",
    "X_test_motion_padded = tf.keras.preprocessing.sequence.pad_sequences(X_test_motion, padding='post', dtype='float32')\n",
    "\n",
    "# Convert data to TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(((X_train_spatial_padded, X_train_motion_padded), y_train)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(((X_test_spatial_padded, X_test_motion_padded), y_test)).batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "input_shape_motion = (max_length, 200, 200, 2)\n",
    "input_motion = tf.keras.layers.Input(shape=input_shape_motion)\n",
    "\n",
    "y = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(32, (3, 3), activation='relu'))(input_motion)\n",
    "y = tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling2D((2, 2)))(y)\n",
    "y = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))(y)\n",
    "y = tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling2D((2, 2)))(y)\n",
    "y = tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten())(y)\n",
    "\n",
    "lstm_out = tf.keras.layers.LSTM(128)(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.keras.layers.Reshape((-1, y.shape[-1]))(y)  # Keep the batch dimension dynamic\n",
    "lstm_out = tf.keras.layers.LSTM(128)(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of GPUs: 0\n",
      "Epoch 1/5\n",
      "5/5 [==============================] - 115s 17s/step - loss: 2.9442 - accuracy: 0.3333 - val_loss: 9.0968 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 79s 16s/step - loss: 1.7408 - accuracy: 0.6667 - val_loss: 5.1397 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 69s 14s/step - loss: 0.7275 - accuracy: 0.5556 - val_loss: 5.9264 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 68s 14s/step - loss: 0.6036 - accuracy: 0.7778 - val_loss: 7.8850 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 66s 13s/step - loss: 0.4872 - accuracy: 0.7778 - val_loss: 9.3857 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# Adjust the input shapes to match the expected dimensions\n",
    "input_shape_spatial = (56, 200, 200)\n",
    "input_shape_motion = (max_length, 200, 200, 2)\n",
    "\n",
    "input_spatial = tf.keras.layers.Input(shape=input_shape_spatial)\n",
    "input_motion = tf.keras.layers.Input(shape=input_shape_motion)\n",
    "\n",
    "# Define the CNN model for spatial features\n",
    "cnn_out = tf.keras.layers.Conv2D(32, (3, 3), activation='relu')(input_spatial)\n",
    "cnn_out = tf.keras.layers.MaxPooling2D((2, 2))(cnn_out)\n",
    "cnn_out = tf.keras.layers.Conv2D(64, (3, 3), activation='relu')(cnn_out)\n",
    "cnn_out = tf.keras.layers.MaxPooling2D((2, 2))(cnn_out)\n",
    "cnn_out_flat = tf.keras.layers.Flatten()(cnn_out)\n",
    "\n",
    "# Define the LSTM model for motion features\n",
    "y = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(32, (3, 3), activation='relu'))(input_motion)\n",
    "y = tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling2D((2, 2)))(y)\n",
    "y = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))(y)\n",
    "y = tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling2D((2, 2)))(y)\n",
    "y = tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten())(y)\n",
    "lstm_out = tf.keras.layers.LSTM(128)(y)\n",
    "\n",
    "# Concatenate spatial and motion features\n",
    "combined = tf.keras.layers.concatenate([cnn_out_flat, lstm_out])\n",
    "\n",
    "# Output layer\n",
    "output = tf.keras.layers.Dense(num_classes, activation='softmax')(combined)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=[input_spatial, input_motion], outputs=output)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Pad spatial features to ensure they have the same shape\n",
    "X_train_spatial_padded = tf.keras.preprocessing.sequence.pad_sequences(X_train_spatial, maxlen=56, padding='post', dtype='float32')\n",
    "X_test_spatial_padded = tf.keras.preprocessing.sequence.pad_sequences(X_test_spatial, maxlen=56, padding='post', dtype='float32')\n",
    "\n",
    "# Convert data to TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(((X_train_spatial_padded, X_train_motion_padded), y_train)).batch(2).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(((X_test_spatial_padded, X_test_motion_padded), y_test)).batch(2).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print('No. of GPUs:', len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "# Train the model\n",
    "history= model.fit(train_dataset, epochs=5, validation_data=test_dataset)\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc:  77.77777910232544 %\n",
      "Test Loss:  48.71918857097626 %\n"
     ]
    }
   ],
   "source": [
    "print('Test Acc: ', max(history['accuracy'])*100, \"%\")\n",
    "print('Test Loss: ', min(history['loss'])*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
